{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TyMill/experiments_ideas/blob/main/rl_eg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the SARSA algorithm to train an agent to determine the optimal price for a product based on the prices of its competitors. The environment is defined by the `Environment` class, which takes in a list of prices representing the competition's prices. The agent, defined by the `SARSAgent` class, interacts with the environment by choosing an action (i.e. setting a price) and receiving a reward based on the profit made from that action. The agent learns from this experience by updating its Q-values for the current state-action pair using the SARSA update rule. After training for a specified number of episodes, the code prints out the optimal price determined by the agent.\n",
        "\n",
        "Note: the optimal price may be lower than the competition price, but it's a good practice to test your strategy with different prices, and different scenarios."
      ],
      "metadata": {
        "id": "uknkJKiqUjTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self, prices):\n",
        "        self.prices = prices\n",
        "        self.num_prices = len(prices)\n",
        "\n",
        "    def transition(self, state, action):\n",
        "        price = self.prices[action]\n",
        "        demand = np.random.normal(100, 15)\n",
        "        revenue = demand * price\n",
        "        cost = demand * 10\n",
        "        profit = revenue - cost\n",
        "        next_state = (state[1], price)\n",
        "        return (next_state, profit)\n",
        "\n",
        "    def reset(self):\n",
        "        return (0, self.prices[0])\n",
        "\n",
        "class SARSAgent:\n",
        "    def __init__(self, actions, epsilon=0.1, alpha=0.5, gamma=0.9):\n",
        "        self.q_values = dict()\n",
        "        self.actions = actions\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def learn(self, state, action, next_state, reward, next_action):\n",
        "        q_val = self.get_q_value(state, action)\n",
        "        next_q_val = self.get_q_value(next_state, next_action)\n",
        "        new_q_val = q_val + self.alpha * (reward + self.gamma * next_q_val - q_val)\n",
        "        self.set_q_value(state, action, new_q_val)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.actions)\n",
        "        else:\n",
        "            q_values = [self.get_q_value(state, a) for a in self.actions]\n",
        "            return self.actions[np.argmax(q_values)]\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        if (state, action) not in self.q_values:\n",
        "            self.q_values[(state, action)] = 0\n",
        "        return self.q_values[(state, action)]\n",
        "\n",
        "    def set_q_value(self, state, action, value):\n",
        "        self.q_values[(state, action)] = value\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prices = [50, 55, 60, 65, 70, 75, 80, 85, 90, 95]\n",
        "    env = Environment(prices)\n",
        "    agent = SARSAgent(prices)\n",
        "    num_episodes = 1000\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        action = agent.act(state)\n",
        "\n",
        "        for t in range(100):\n",
        "            next_state, reward = env.transition(state, action)\n",
        "            next_action = agent.act(next_state)\n",
        "            agent.learn(state, action, next_state, reward, next_action)\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    q_values = [agent.get_q_value((0, p), p) for p in prices]\n",
        "    optimal_price = prices[np.argmax(q_values)]\n",
        "    print(\"Optimal price:\", optimal_price)\n"
      ],
      "metadata": {
        "id": "bTKCtBFtUaAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df z xls'ow"
      ],
      "metadata": {
        "id": "hY4JrK495xyF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC_TFecY5Xnn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Set the path to the folder containing the Excel files\n",
        "path = 'path/to/folder'\n",
        "\n",
        "# Create an empty list to store the DataFrames\n",
        "data_frames = []\n",
        "\n",
        "# Iterate through the files in the folder\n",
        "for file_name in os.listdir(path):\n",
        "    # Check if the file is an Excel file\n",
        "    if file_name.endswith('.xlsx'):\n",
        "        # Read the Excel file into a DataFrame\n",
        "        df = pd.read_excel(os.path.join(path, file_name))\n",
        "        # Append the DataFrame to the list\n",
        "        data_frames.append(df)\n",
        "\n",
        "# Concatenate all of the DataFrames together\n",
        "final_df = pd.concat(data_frames)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dict z csv"
      ],
      "metadata": {
        "id": "2Rs8VAVD5-Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('path/to/file.csv')\n",
        "\n",
        "# Convert the DataFrame to a dictionary\n",
        "data_dict = df.to_dict()\n",
        "\n",
        "data_dict = df.to_dict(orient='records')\n"
      ],
      "metadata": {
        "id": "KKbgpwaW6Ana"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "slownik"
      ],
      "metadata": {
        "id": "lGsFnw6W7wT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict = {}\n",
        "for i in range(1,5001):\n",
        "    my_dict[f'EC{i}'] = f'Fandom{i}'\n"
      ],
      "metadata": {
        "id": "_VX2zv03730X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "julia xg boost"
      ],
      "metadata": {
        "id": "I-yI-3lc_G6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using XGBoost\n",
        "using RDatasets\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = dataset(\"datasets\", \"iris\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_indices = sample(1:nrow(iris), Int(0.8 * nrow(iris)))\n",
        "train = iris[train_indices, :]\n",
        "test = iris[setdiff(1:nrow(iris), train_indices), :]\n",
        "\n",
        "# Define the training data and labels\n",
        "train_data = train[:, 1:4]\n",
        "train_labels = train[:, :Species]\n",
        "\n",
        "# Define the test data and labels\n",
        "test_data = test[:, 1:4]\n",
        "test_labels = test[:, :Species]\n",
        "\n",
        "# Train the model\n",
        "model = xgboost(train_data, train_labels, objective = \"multi:softprob\", num_class = 3)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = predict(model, test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = mean(argmax(predictions, 2) .== argmax(test_labels, 2))\n",
        "println(\"Accuracy: $accuracy\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "using XGBoost\n",
        "using DataFrames\n",
        "using MLJ\n",
        "using Random\n",
        "using Flux\n",
        "using Flux.Data.MNIST\n",
        "using MLJModels\n",
        "using MLJFlux\n",
        "using MLJTuning\n",
        "using MLJTuning.GridSearch\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "parameter_grid = Dict(\n",
        "    :eta => [0.1, 0.01, 0.001],\n",
        "    :max_depth => [2, 3, 4, 5],\n",
        "    :subsample => [0.7, 0.8, 0.9],\n",
        "    :colsample_bytree => [0.7, 0.8, 0.9],\n",
        "    :alpha => [0, 1, 2],\n",
        "    :lambda => [0, 1, 2],\n",
        "    :num_round => [10, 20, 30]\n",
        ")\n",
        "\n",
        "# Define the XGBoost model\n",
        "model = XGBoostClassifier()\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(model, parameter_grid)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "MLJ.fit!(grid_search, train_data, train_labels)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "println(\"Best parameters: \", best_params)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = MLJ.predict(grid_search, test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = mean(predictions .== test_labels)\n",
        "println(\"Accuracy: $accuracy\")\n"
      ],
      "metadata": {
        "id": "utX0Pjt3_JGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example uses the Q-learning algorithm to train an agent to navigate a grid in the \"FrozenLake-v0\" environment from OpenAI Gym. The agent starts in a random state and takes actions based on the Q-values of the state-action pairs. The Q-values are updated using the Q-learning update rule, which is based on the observed reward and the maximum Q-value of the next state. The agent's performance is evaluated by measuring the total reward it receives during a test episode, after training.\n",
        "\n",
        "This is a basic example of RL in python, but it is used to demonstrate the concept of RL. There are many more complex algorithms exist that can be used to solve different types of problems."
      ],
      "metadata": {
        "id": "8Rn6s4apBC_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class MyEnv:\n",
        "    def __init__(self, prices, demand_params):\n",
        "        self.prices = prices\n",
        "        self.demand_params = demand_params\n",
        "        self.reset()\n",
        "\n",
        "    def transition(self, state, action):\n",
        "        price = self.prices[action]\n",
        "        demand = np.random.normal(self.demand_params[0], self.demand_params[1])\n",
        "        revenue = price * demand\n",
        "        next_state = revenue\n",
        "        return next_state, revenue\n",
        "        \n",
        "    def reset(self):\n",
        "        self.state = None\n",
        "        return self.state\n",
        "    \n",
        "class QLearningAgent:\n",
        "    def __init__(self, actions, epsilon=0.1, alpha=0.5, gamma=0.9):\n",
        "        self.q = defaultdict(lambda: [0.0, 0.0])\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.actions = actions\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        max_q_next = max(self.q[next_state])\n",
        "        q_val = self.q[state][action]\n",
        "        q_val += self.alpha * (reward + self.gamma * max_q_next - q_val)\n",
        "        self.q[state][action] = q_val\n",
        "\n",
        "    def act(self, state, epsilon=None):\n",
        "        if epsilon is None:\n",
        "            epsilon = self.epsilon\n",
        "        if random.random() < epsilon:\n",
        "            return random.choice(self.actions)\n",
        "        else:\n",
        "            q_values = self.q[state]\n",
        "            return self.actions[np.argmax(q_values)]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prices = [10, 20, 30, 40, 50]\n",
        "    demand_params = [100, 20]\n",
        "    env = MyEnv(prices, demand_params)\n",
        "\n",
        "    agent = QLearningAgent(range(len(prices)))\n",
        "\n",
        "    num_episodes = 1000\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        action = agent.act(state)\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            next_state, reward = env.transition(state, action)\n",
        "            agent.learn(state, action, reward, next_state)\n",
        "            total_reward += reward\n",
        "\n",
        "            action = agent.act(next_state)\n",
        "            state = next_state\n",
        "\n",
        "            if env.termination(state):\n",
        "                break\n",
        "    print(agent.q)\n",
        "    optimal_price = prices[np.argmax(agent.q[None])]\n",
        "    print(f'Optimal price: {optimal_price}')\n"
      ],
      "metadata": {
        "id": "4lTgTjLFTByP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# Define the Q-table and its initial values\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Define the hyperparameters\n",
        "num_episodes = 10000\n",
        "learning_rate = 0.8\n",
        "max_steps = 99\n",
        "gamma = 0.95\n",
        "\n",
        "# Train the agent\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rewards = 0\n",
        "    for step in range(max_steps):\n",
        "        # Choose an action based on the current state\n",
        "        action = np.argmax(q_table[state, :] + np.random.randn(1, env.action_space.n) * (1. / (episode + 1)))\n",
        "\n",
        "        # Take the action and observe the new state, reward, and whether the episode is done\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update the Q-value for the current state-action pair\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action])\n",
        "        rewards += reward\n",
        "        state = new_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(\"Average reward:\", rewards / 1000)\n",
        "\n",
        "# Test the agent\n",
        "state = env.reset()\n",
        "done = False\n",
        "rewards = 0\n",
        "for step in range(max_steps):\n",
        "    # Choose the action with the highest Q-value\n",
        "    action = np.argmax(q_table[state, :])\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    rewards += reward\n",
        "    if done:\n",
        "        break\n",
        "print(\"Total reward:\", rewards)\n"
      ],
      "metadata": {
        "id": "04J5cyB7BDQN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}